{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.webdriver.chrome.options import Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Crawl Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def crawl_and_save_to_file(url, output_file):\n",
    "    edge_options = Options()\n",
    "    edge_options.add_argument(\"--headless\")\n",
    "    edge_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36\")\n",
    "    driver = webdriver.Edge(options=edge_options)\n",
    "    driver.get(url)\n",
    "    time.sleep(2)  \n",
    "    try:\n",
    "            response = requests.get(url, verify=False)\n",
    "            if response.headers.get('Content-Type') == 'application/pdf':\n",
    "                with open(output_file, 'wb') as pdf_file:\n",
    "                    pdf_file.write(response.content)\n",
    "                driver.quit()\n",
    "                return\n",
    "    except Exception as e:\n",
    "            print(f\"Có lỗi xảy ra khi tải xuống file PDF: {e}\")\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    wr_bottom = soup.find('div', id='wr_bottom')\n",
    "    if wr_bottom:\n",
    "        wr_bottom.decompose() \n",
    "    box_gray = soup.find('div', class_='box_gray bottom10 list-article-bottom')\n",
    "    if box_gray:\n",
    "        for p in box_gray.find_all('p'):\n",
    "            p.decompose()  \n",
    "    box_others = soup.find_all('div', class_='box_other pad10 clearfix')\n",
    "    for box_other in box_others:\n",
    "        box_other.clear() \n",
    "    modal_dialogs = soup.find_all('div', class_='modal-dialog')\n",
    "    for modal_dialog in modal_dialogs:\n",
    "        modal_dialog.clear()  \n",
    "    popup_confirm_notify=soup.find('div',class_='popup-confirm-notify')\n",
    "    if popup_confirm_notify:\n",
    "        popup_confirm_notify.clear()\n",
    "    binhluan = soup.find('div', class_='binhluan')\n",
    "    if binhluan:\n",
    "        binhluan.decompose() \n",
    "    other=soup.find('div',class_='other')\n",
    "    if other:\n",
    "        other.clear()\n",
    "    AsideFirstZone_206=soup.find('div', id='AsideFirstZone_206')\n",
    "    if AsideFirstZone_206:\n",
    "        AsideFirstZone_206.clear()\n",
    "    margin_5pxs=soup.find_all('div', style=\"margin:5px;border-top: 1px dashed #DDDDDD\")\n",
    "    for margin_5px in margin_5pxs :\n",
    "        margin_5px.clear()\n",
    "    col_md_7=soup.find('div',class_='col-md-7 col-sm-5')\n",
    "    if col_md_7:\n",
    "        col_md_7.clear()\n",
    "    time_post=soup.find('p',class_='time-post text-change-size')\n",
    "    if time_post:\n",
    "        time_post.clear()\n",
    "    portlet_title=soup.find('h1',class_='portlet-title')\n",
    "    if portlet_title: \n",
    "        portlet_title.clear()\n",
    "    titlebar_clearfixs=soup.find_all('div',class_='titlebar clearfix')\n",
    "    for titlebar_clearfix in titlebar_clearfixs:\n",
    "         titlebar_clearfix.clear()\n",
    "    rows=soup.find_all('div',class_='col-xs-6 col-sm-4')\n",
    "    for row in rows:\n",
    "        row.clear()\n",
    "    zonepage_r=soup.find('div',class_='zonepage-r')\n",
    "    if zonepage_r:\n",
    "        zonepage_r.clear()\n",
    "    message_hidden=soup.find('div',class_='message hidden')\n",
    "    if message_hidden:\n",
    "        message_hidden.clear()\n",
    "    commentform_clearfix=soup.find('div',class_='commentform clearfix')\n",
    "    if commentform_clearfix:\n",
    "        commentform_clearfix.clear()\n",
    "    pnlCommentDialog=soup.find('div',id='pnlCommentDialog')\n",
    "    if pnlCommentDialog:\n",
    "        pnlCommentDialog.clear()\n",
    "    storyothers_clearfix=soup.find('div',class_='storyothers clearfix')\n",
    "    if storyothers_clearfix:\n",
    "        storyothers_clearfix.clear()\n",
    "    box_tinlienquan=soup.find('div',class_='content-right column')\n",
    "    if box_tinlienquan:\n",
    "        box_tinlienquan.clear()\n",
    "    col_sm=soup.find('div',class_='col-sm-8 col-md-6')\n",
    "    if col_sm:\n",
    "        col_sm.clear()\n",
    "    post_relate=soup.find('div',class_='post-relate')\n",
    "    if post_relate:\n",
    "        post_relate.clear()\n",
    "    col_12=soup.find('div',class_='bg-white py-3 py-lg-4')\n",
    "    if col_12:\n",
    "        col_12.clear()\n",
    "    item_news_other=soup.find('div',class_='item_news_other')\n",
    "    if item_news_other:\n",
    "        item_news_other.clear()\n",
    "    panel_body_other=soup.find('div',class_='panel-body other-news')\n",
    "    if panel_body_other:\n",
    "        panel_body_other.clear()\n",
    "    box_related_news=soup.find('div',class_='box-related-news')\n",
    "    if box_related_news:\n",
    "        box_related_news.clear()\n",
    "    timeline_secondary=soup.find('div',class_='timeline secondary')\n",
    "    if timeline_secondary:\n",
    "        timeline_secondary.clear()\n",
    "    note_btn=soup.find('div',class_='note-btn')\n",
    "    if note_btn:\n",
    "        note_btn.clear()\n",
    "    panel_panel_default=soup.find('div',class_='col-xs-12 col-sm-12 col-md-12')\n",
    "    if panel_panel_default:\n",
    "        panel_panel_default.clear()\n",
    "    relate_news=soup.find('div',class_='relate-news')\n",
    "    if relate_news:\n",
    "        relate_news.clear()\n",
    "    section_maybelike=soup.find('section',class_='section-3 maybelike')\n",
    "    if section_maybelike:\n",
    "        section_maybelike.clear()\n",
    "    section_3_d_none=soup.find('section',class_='section-3 d-none')\n",
    "    if section_3_d_none:\n",
    "        section_3_d_none.clear()\n",
    "    section_1_section_3=soup.find('section',class_='section-1 section-3')\n",
    "    if section_1_section_3:\n",
    "        section_1_section_3.clear()\n",
    "    box_tinkhac=soup.find('div',class_='box_tinkhac')\n",
    "    if box_tinkhac :\n",
    "        box_tinkhac.clear()\n",
    "    more_news=soup.find('div',class_='tin-van')\n",
    "    if more_news:\n",
    "        more_news.clear()\n",
    "    tab_content=soup.find('div',class_='tab-content')\n",
    "    if tab_content:\n",
    "        tab_content.clear()\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        headers = []\n",
    "        for i in range(1, 2):\n",
    "            headers.extend(soup.find_all(f'h{i}'))\n",
    "        for header in headers:\n",
    "            file.write(header.get_text(strip=True) + '\\n')  \n",
    "        footers = soup.find_all(lambda tag: tag.name == 'footer' or \n",
    "                                (tag.get('id') and 'footer' in tag.get('id')) or \n",
    "                                (tag.get('class') and any('footer' in cls for cls in tag.get('class'))))\n",
    "\n",
    "        footer_elements = [footer for footer in footers]\n",
    "        paragraphs = soup.find_all('p')\n",
    "        for paragraph in paragraphs:\n",
    "            if not any(paragraph in footer.find_all('p') for footer in footer_elements):\n",
    "                file.write(paragraph.get_text(strip=True) + '\\n') \n",
    "        body_text_div = soup.find('div', class_='bodytext margin-bottom-lg', id='news-bodyhtml')\n",
    "        if body_text_div:\n",
    "            file.write(body_text_div.get_text(strip=True) + '\\n')\n",
    "    with open(output_file, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "    if len(lines) < 6:\n",
    "        spans = soup.find_all('span')\n",
    "        with open(output_file, 'a', encoding='utf-8') as file: \n",
    "            for span in spans:\n",
    "                if not any(span in footer.find_all('span') for footer in footer_elements):\n",
    "                    file.write(span.get_text(strip=True) + '\\n')\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hdang\\.virtualenvs\\machinelearning-lTTH8rYd\\Lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'tulieuvankien.dangcongsan.vn'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã lưu nội dung trang web vào file: ../Test_DUAN/data_content/dong_tu_tich_cuc/khai_quat_hoa/uu_tien_1.txt\n",
      "File chỉ có 4 dòng, tiếp tục tìm thẻ <span>...\n",
      "Đã thêm nội dung từ thẻ <span> vào file: ../Test_DUAN/data_content/dong_tu_tich_cuc/khai_quat_hoa/uu_tien_1.txt\n",
      "Đã lưu nội dung trang web vào file: ../Test_DUAN/data_content/dong_tu_tich_cuc/khai_quat_hoa/uu_tien_1.txt\n"
     ]
    }
   ],
   "source": [
    "# URL của trang web bạn muốn crawl\n",
    "\n",
    "url = f'https://tulieuvankien.dangcongsan.vn/c-mac-angghen-lenin-ho-chi-minh/ho-chi-minh/nghien-cuu-hoc-tap-tu-tuong/mot-so-van-de-phuong-phap-luan-va-phuong-phap-nghien-cuu-ve-ho-chi-minh-2080'\n",
    "# Kiểm tra nếu URL có chứa 'pdf' thì sẽ lưu với tên uu_tien_5.pdf\n",
    "if 'pdf' in url:\n",
    "    output_file = '../Test_DUAN/data_content//uu_tien_1.pdf'\n",
    "else:\n",
    "    output_file = '../Test_DUAN/data_content/dong_tu_tich_cuc/khai_quat_hoa/uu_tien_1.txt'\n",
    "# Gọi hàm với URL và file đầu ra\n",
    "crawl_and_save_to_file(url, output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machinelearning-lTTH8rYd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
